# src/model_serving/service.py

import os
import traceback

import bentoml
import numpy as np
from bentoml.io import JSON
from pydantic import BaseModel

# --- 1. IO (GiriÅŸ/Ã‡Ä±kÄ±ÅŸ) Modelleri ---
# Bu kÄ±sÄ±m, API'nin ne tÃ¼r veri beklediÄŸini ve dÃ¶ndÃ¼receÄŸini tanÄ±mlar.


class InputData(BaseModel):
    data: list[list[float]]


class OutputData(BaseModel):
    prediction: list[float]


class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    model_version: str
    target_stage: str
    mlflow_uri: str
    message: str


class ReloadResponse(BaseModel):
    status: str
    old_version: str
    new_version: str
    message: str


# --- 2. Global DeÄŸiÅŸkenler ---
# Bu deÄŸiÅŸkenler, servis Ã§alÄ±ÅŸtÄ±ÄŸÄ± sÃ¼rece hafÄ±zada tutulur.

# MLflow sunucusunun adresi ortam deÄŸiÅŸkeninden alÄ±nÄ±r.
MLFLOW_URI = os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow:5000")

# Hangi modelin hangi aÅŸamasÄ±nÄ± yÃ¼kleyeceÄŸimizi ortam deÄŸiÅŸkeninden Ã¶ÄŸreniriz.
# Bu, production ve canary servislerinin farklÄ± modeller yÃ¼klemesini saÄŸlar.
MODEL_TAG_STR = os.environ.get(
    "BENTOML_MODEL_TAG", "models:/isolation_forest_model/Production"
)
MODEL_NAME = MODEL_TAG_STR.split("/")[1]
TARGET_STAGE = MODEL_TAG_STR.split("/")[2]

# YÃ¼klenen model ve versiyonu burada saklanÄ±r.
model = None
model_version = "none"

# --- 3. Ana Model YÃ¼kleme Fonksiyonu ---
# Bu fonksiyon, servisin kalbidir. MLflow'dan doÄŸru modeli bulur ve yÃ¼kler.


def load_model_from_mlflow():
    """
    Ortam deÄŸiÅŸkeninde belirtilen model ve aÅŸamayÄ± MLflow'dan yÃ¼kler.
    Model bulunamazsa hata vermeden devam eder.
    """
    global model, model_version

    try:
        import mlflow

        mlflow.set_tracking_uri(MLFLOW_URI)
        client = mlflow.MlflowClient()

        print(f"\n{'='*60}")
        print(f"ðŸ” Model aranÄ±yor: '{MODEL_NAME}', Hedef AÅŸama: '{TARGET_STAGE}'")
        print(f"ðŸ“ MLflow URI: {MLFLOW_URI}")
        print(f"{'='*60}")

        versions = client.get_latest_versions(MODEL_NAME, stages=[TARGET_STAGE])

        if not versions:
            print(f"âš ï¸  '{TARGET_STAGE}' aÅŸamasÄ±nda model bulunamadÄ±.")
            model = None
            model_version = "none"
            return

        latest_version = versions[0]
        model_uri = f"models:/{MODEL_NAME}/{latest_version.version}"
        print(f"ðŸŸ¢ '{TARGET_STAGE}' modeli bulundu: Versiyon {latest_version.version}")

        print("ðŸ“¦ Model yÃ¼kleniyor...")
        model = mlflow.pyfunc.load_model(model_uri)
        model_version = latest_version.version
        print(f"âœ… Model baÅŸarÄ±yla yÃ¼klendi! Versiyon: {model_version}")

    except Exception as e:
        print(f"âŒ Model yÃ¼kleme sÄ±rasÄ±nda detaylÄ± hata oluÅŸtu: {e}")
        traceback.print_exc()
        model = None
        model_version = "none"


# --- 4. Servis BaÅŸlangÄ±cÄ± ---
# Konteyner ayaÄŸa kalktÄ±ÄŸÄ±nda bu kodlar bir kez Ã§alÄ±ÅŸÄ±r.

print("\n" + "ðŸš€" * 30)
print(f"ðŸš€ VitalWatch BentoML Servisi BaÅŸlatÄ±lÄ±yor... (Hedef: {TARGET_STAGE})")
print("ðŸš€" * 30 + "\n")
load_model_from_mlflow()

if model is None:
    print("\n" + "âš ï¸ " * 30)
    print("âš ï¸  SERVÄ°S MODEL OLMADAN BAÅžLATILDI!")
    print(f"âš ï¸  '{TARGET_STAGE}' aÅŸamasÄ±nda yÃ¼klenecek model bulunamadÄ±.")
    print("âš ï¸ " * 30 + "\n")

# --- 5. BentoML Servis ve API TanÄ±mlarÄ± ---

svc = bentoml.Service("vitalwatch_service")


@svc.api(input=JSON(pydantic_model=InputData), output=JSON(pydantic_model=OutputData))
def predict(input_data: InputData) -> OutputData:
    """Anomali tahmini yapar."""
    global model
    if model is None:
        print("âŒ Tahmin yapÄ±lamÄ±yor: Model yÃ¼klÃ¼ deÄŸil.")
        return OutputData(prediction=[-999.0])

    try:
        arr = np.array(input_data.data)
        preds = model.predict(arr)
        preds_list = preds.tolist()
        print(f"âœ… Tahmin baÅŸarÄ±lÄ±: {len(preds_list)} sonuÃ§ Ã¼retildi.")
        return OutputData(prediction=preds_list)
    except Exception as e:
        print(f"âŒ Tahmin sÄ±rasÄ±nda hata: {e}")
        traceback.print_exc()
        return OutputData(prediction=[-999.0])


@svc.api(input=JSON(), output=JSON(pydantic_model=ReloadResponse))
def reload_model(input_data: dict) -> ReloadResponse:
    """Modeli MLflow'dan yeniden yÃ¼kler."""
    global model_version
    old_version = str(model_version)

    print("\n" + "ðŸ”„" * 30)
    print("ðŸ”„ Model Yeniden YÃ¼kleniyor...")
    load_model_from_mlflow()

    new_version = str(model_version)
    success = model is not None

    return ReloadResponse(
        status="success" if success else "failed",
        old_version=old_version,
        new_version=new_version,
        message=(
            "Model reloaded successfully"
            if success
            else f"No model found for stage '{TARGET_STAGE}'"
        ),
    )


@svc.api(input=JSON(), output=JSON(pydantic_model=HealthResponse))
def health(input_data: dict) -> HealthResponse:
    """Servisin ve modelin saÄŸlÄ±k durumunu kontrol eder."""
    is_healthy = model is not None
    message = "Service is healthy and model is loaded"
    if not is_healthy:
        message = f"Service is running but NO MODEL LOADED for stage '{TARGET_STAGE}'!"

    return HealthResponse(
        status="healthy" if is_healthy else "no_model",
        model_loaded=is_healthy,
        model_version=str(model_version),
        target_stage=TARGET_STAGE,
        mlflow_uri=MLFLOW_URI,
        message=message,
    )
